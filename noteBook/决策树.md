决策树
=======================
决策树是一种分类与回归方法，和人类理解分类的方式非常相似。

### 决策树模型

##### 决策树组成

> 决策树是一个树形结构，由边和节点组成，其节点主要分为中间节点和叶子节点，中间每个节点对应一个属性（属性集合），每个边对应一个属性值，每个叶子节点表示一个类

- 中间节点：表示某一个特征或者属性，该样本特征向量在该节点按照该属性进行划分
- 边：表示属性的值，符合该属性的样本分配到该边指向的节点
- 叶子节点：表示一个类

##### 分类方法

分类时，假设实例特征向量为$(x^{(1)},...,x^{(n)})$，从根节点开始，根据实例对应该节点属性的值进行分配，分配到节点的某个孩子节点中；如果递归往复直到达到叶子节点，得到实例的类。

##### 决策树的训练

训练集合
$$
\begin{aligned}
&D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_n)\}\\
where\ &x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T\\
&y_i\in\{1,2,...,K\}
\end{aligned}
$$
个人觉得可以将构建决策树理解成如下的方式

```c++
createTree(dataSet, root){
    if almostClassified(dataSet){
        root.child = createLeaf(dataSet);
    }else{
        fea = findAGoodFeature(dataSet);
        newDataSet[0], newDataSet[1], ..., newDataSet[p] = splitDataSet(dataSet, fea);
        for(i = 0; i <= p; i++)
           root.child[i] = create(newdataSet[i])
    }
}
```

由此可见，在构建决策树的过程中最需要以下工作

1. `almostClassified(dataSet)`给定一个data set是否已符合成为叶子节点（分类基本相同）的条件
2. `findAGoodFeature(dataSet)`给定一个寻找较好的划分属性的方法
3. `splitDataSet(dataSet, fea)`根据找到的feature划分data set

### 特征选择

> 下面的特征选择只是特征选择一种方式，只要某种方法能够找出使得数据集不确定性降低最大的特征，该方法理论上就能用作为特征选择的方法，而**不只是信息增益比**。

在选取中间节点的分类特征时，我们想让该特征分类能力尽可能的强。那么什么才叫做分类能力尽可能强呢，下面需要引入信息增益的概念。

##### 熵

熵是表示随机变量不确定性的度量，假设$X$概率分布为$P(X=x_i) =p_i,   i=1,2,..,n$，则$X$的熵为
$$
H(X)=-\sum_{i=1}^N(p_ilog(p_i))\\
H(X)=0\ when\ p_i=0
$$
可以验证得到当$H(X=x_i)=1/N, i=1,2,...,N$时，$H(X)$取得最大值$log(n)$，而且当$X$分布越不平均时，$H(X)$越大，极端情况下若$\exist j, p_j=1$则$H(X)=0$。这直观上也很好理解，假如一个袋子里面1/2是白球，1/2是黑球，摸出一个球，很难猜测到底摸出的白球还是黑球。但是如果袋子里面99/100是白球，1/100是黑球，则摸出一个球则几乎可以确定是白球，确定性很高。

##### 条件熵

条件熵也很好理解，对于随机变量$(X,Y)$的联合概率分布$P(X=x_i, Y=y_i) = p_{ij}, i=1,2,...,n;j=1,2...,m$，条件熵$H(Y|X)$表示，在随机变量$X$已知的情况下Y的不确定性，记作
$$
H(Y|X)=\sum_{i=1}^np_i(H(Y|X=x_i))\\
where\ p_i=P(X=x_i), i=1,2,...,n
$$

##### 信息增益

信息增益表示得知特征$X$的信息使得类$Y$的信息不确定性减少的程度。记特征$A$对数据集$D$的信息增益为$g(D,A)$，则
$$
g(D,A)=H(D)-H(D|A)
$$
信息增益越大，表示$A​$使得数据集$D​$不确定性减少越多，越适合用作分类特征。

信息增益的计算过程：训练数据集为$D​$，$|D|​$表示其样本个数。设有 $K​$个类$C_k​$，$k=1,2,...,K​$，$|C_k|​$为属于类$C_k​$的样本个数。设特征$A​$有$n​$个不同得取值$\{a_1,a_2,...,a_n\}​$，根据特征$A​$的取值将划分为个子集$D_1,D_2,...,D_n​$，$|D_i|​$为$D_i​$的样本个数。记子集$D_i​$中属于类$C_k​$的样本集合为$D_{ik}​$，$|D_{ik}|​$为$|D_{ik}|​$样本个数。于是信息增益得算法如下：

1. 计算数据集$D$得经验熵

$$
H(D)=-\sum_{i=1}^K\frac{|C_k|}{|D|}log_2(\frac{|C_k|}{|D|})
$$

2. 计算特征$A$对数据集D的条件熵$H(D|A)$

$$
H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2(\frac{|D_{ik}|}{|D_i|})
$$

3. 计算信息增益比

$$
g(D,A)=H(D)-H(D,A)
$$

##### 信息增益比

如果采用信息增益作为划分特征，则模型会偏向于选择值较多的特征（因为此时$log(n)$中$n$比较大），可以使用信息增益比对这一问题进行矫正。特征$A$对训练集$D$的信息增益比$g_R(D,A)$定义为：
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\
where\ H_A(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2(\frac{|D_i|}{|D|})
$$
这样做的原因是考虑了比值，而不是考虑净减少量。对于特征值较少的特征，其$H_A(D)$也较小，这样可以获得平衡。

##### 特征选择和叶子节点

现在特征选择的条件就很清楚了，选用信息增益比最大的特征作为划分特征。另外，根据信息增益比可以决定什么时候划分叶子节点。我们可以设置一个阈值$\epsilon$，当我们找到的最大的信息增益比小于$\epsilon$时，该中间节点就不再继续划分，而是成为一个叶子节点，其类别由势力中最大的类$C_k$决定。

##### ID3和C4.5算法

上面是两种简单的决策树构建方法，其具体过程就和以上介绍的算法相同（ID3用的信息增益，C4.5用的信息增益比），两者都没有减枝。

### 减枝

