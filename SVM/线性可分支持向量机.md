SVM——线性可分支持向量机
===================
# SVM思想
考虑二分类问题，对于线性可分的数据集，如果采用感知机模型，其目的是找到一个超平面将两类分开。但是显然这样的平面不一定只存在一个，那么需要找到一个其中的某个“最好”的平面，使得**该超平面到两类点的最小距离最大**。
# 数据集
数据集用$D$表示，样本共有$N$个，样本特征记作$x_i, i=1,1,2,...,N$，对应的标签为$y_i, i=1,1,2,...,N$，对于正例，$y_i==1$，对于反例$y_i==-1$。
# 函数间隔和几何间隔
这时两种度量样本（数据集）到超平面距离的值。记分类平面表示为
$$
wx+b=0
$$
记样本函数间隔为
$$
\gamma_i = y_i(wx_i+b)
$$
则数据集函数间隔为
$$
\gamma = min(\gamma_i), i=1,...,N
$$
样本几何间隔为
$$
\frac{\gamma_i}{||w||}
$$
数据集几何间隔为
$$
min(\frac{\gamma_i}{||w||}), i=1,...,N
$$
显然函数间隔或者几何间隔越小（都为正，因为分类正确时，分类平面$wx_i+b<0$时，$y_i<0$，其积大于0），样本点距离分类平面越近。
# 优化原始问题
svm的思想是**该超平面到两类点的最小距离最大**，即：
$$
\begin{aligned}
max\ &\frac{\gamma}{||w||}\\
subject\ to\ &y_i(wx_i+b)\geq \gamma,\ i=1,...,N
\end{aligned}
$$
上述的式子即最大化几何间隔，其中$\gamma$是数据集函数间隔。现在考虑如果已经解出该平面$wx+b=0$，则将w和b变为原来的$\lambda$倍，则函数间隔变为$\frac{\gamma}{\lambda}$，但此时$wx+b=0$代表的平面依然不变。所以其实$\gamma$的值并不重要，我们可以取$\gamma=1$，则优化问题变为
$$
\begin{aligned}
max\ &\frac{1}{||w||}\\
subject\ to\ &y_i(wx_i+b)\geq 1,\ i=1,...,N
\end{aligned}
$$
等价于优化0
$$
\begin{aligned}
min\ &\frac{1}{2}||w||^2\\
subject\ to\ &1-y_i(wx_i+b)\leq 0,\ i=1,...,N
\end{aligned}
$$
# 拉格朗日最小乘数法
### 原始问题
对于以下的优化问题1
$$
\begin{aligned}
min\ &f(x)\\
subject\ to\ &c_i(x)\leq 0,\ i=1,...,P\\
& h_i(x)=0 ,\  i=1,...,Q
\end{aligned}
$$
记函数
$$
L(x,\alpha,\mu) = f(x)+\sum^P_{p=1}\alpha_pc_i(x)+\sum^Q_{q=1}\mu_qh_i(x)
$$
其中$\alpha_p\geq0$。则
$$
\max_{\alpha,\mu}L(x,\alpha,\mu)=f(x)
$$
$\max_{\alpha,\mu}L(x,\alpha,\mu)$可以理解为，确定x时，$L(x,\alpha,\mu)$通过调整$\alpha,\mu$，获得的最大值。该值与$f(x)$是相同的。
>**证明**
>当$c_i(x) h_i(x)$符合上述的约束条件时，显然$\alpha==0,\mu==0$时$L(x,\alpha,\mu)$取得最大值，此时$L(x,\alpha,\mu)=f(x)$。
>否则，$\exists i, c_i(x)>0$，此时取$\alpha_i=+
>\infty$，函数值为$+\infty$；或者$\exists i, h_i(x)!=0$，取$\mu_i=(+\infty\ when\ h_i(x)>0;-\infty\ when\ h_i(x)<0)$，函数值为$+\infty$。

所以$\min\limits_{x}\ f(x)=\min\limits_{x}\max\limits_{\alpha,\mu}L(x,\alpha,\mu)$，$\max\limits_{\alpha,\mu}L(x,\alpha,\mu)$即拉格朗日乘数法的原始问题2。
### KKT条件
对于上述优化问题1，如果存在最优解，则最优解的$x,\alpha,\mu$应该[一定满足KKT条件](https://zhuanlan.zhihu.com/p/38163970)，即
$$
\begin{aligned}
\nabla_xL(x,\alpha,\mu)=0\\
\alpha_ic_i(x)=0\\
c_i(x)\leq0\\
\alpha_i\geq0\\
h_j(x)=0
\end{aligned}
$$
### 对偶问题
记上述优化的原始问题的对偶问题为
$$
\max_{\alpha,\mu}\min_xL(x,\alpha,\mu)
$$
当满足Slater定理时，该问题的解点与原始问题2的解点相同。
### 转化过程
![3fd224f5c1f421633426616853e34b19.jpeg](en-resource://database/544:1)

# 优化对偶问题
通过上面的分析，求解优化问题1相当于求解优化问题
$$
\begin{aligned}
\max_{\alpha,\mu}\min_xL(x,\alpha,\mu)\\
\alpha_i\geq0
\end{aligned}
$$
代入优化0，即优化
$$
\begin{aligned}
\max_{\alpha}\min_{w,b}L(w,b,\alpha)\\
\alpha_i\geq0
\end{aligned}
$$
首先第一步，求$\min\limits{w,b}L(w,b,\alpha)$
对w,b求偏导，使其等于0，则得到
$$
\begin{aligned}
w=\sum_{i=1}^N\alpha_iy_ix_i\\
\sum_{i=1}^N\alpha_iy_i=0
\end{aligned}
$$
代入$L(w,b,\alpha)$并化简得到
$$
\min\limits{w,b}L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i
$$
求解$\max_{\alpha,\mu}\min_xL(x,\alpha,\mu)$，即
$$
\begin{aligned}
\max -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i\\
subject\ to\ \sum_{i=1}^N\alpha_iy_i=0\\
\alpha_i \geq 0
\end{aligned}
$$
相当于优化3
$$
\begin{aligned}
\min \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i\\
subject\ to\ \sum_{i=1}^N\alpha_iy_i=0\\
\alpha_i \geq 0
\end{aligned}
$$
计算这个的方法为序列最小化方法SMO，这个后面会讲到。
# $\alpha$和$w,b$
现在假设我们已经得到了上述优化3的解$\alpha$。由于最优解一定符合KKT条件，则有如下限制：
$$
\begin{aligned}
\nabla_xL(w,b,\alpha)=w-\sum_{i=1}^N\alpha_iy_ix_i=0\\
\nabla_bL(w,b,\alpha)=\sum_{i=1}^N\alpha_iy_i=0\\
\alpha_i(y_i(w\cdot x)-1)=0\\
y_i(w\cdot x)-1\geq0\\
\alpha_i\geq0\\
\end{aligned}
$$
得到
$$
\begin{aligned}
w&=\sum_{i=1}^N\alpha_iy_ix_i\\
b&=y_j-\sum_{i=1}^N\alpha_iy_i(x_i\cdot x_j)
\end{aligned}
$$
其中用于计算b的$y_j$取任意一个$\alpha_j>0$的$j$。最终得到的分类平面表达式为
$$
\sum_{i=1}^N\alpha_iy_i(x_i\cdot x)+b=0
$$