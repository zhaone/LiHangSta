SVM——序列最小最优算法
======================
回顾下线性支持向量机最终需要求解的问题
$$
\begin{aligned}
\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)+\sum_{i=1}^N\alpha_i\\
subject\ to\ \sum_{i=1}^N\alpha_iy_i=0\\
0\leq\alpha_i\leq C
\end{aligned}
$$
这时一个凸二次规划问题（我也不知道为什么是凸二次规划问题。。。），有全局最优解，采用[序列最小最优化SMO](https://ieeexplore.ieee.org/abstract/document/4731075/)算法求解。
### 基本思路
由于KKT条件是满足最优解的充分必要条件，如果所有变量都满足此条件，那么这个最优化问题的解就得到了。
每次选定两个变量，固定其他所有变量，优化这两个变量的值，使得原始目标函数值变的更小。在此优化中，假设选定的两个变量为$\alpha_1,\alpha_2$，则$\alpha_3,\alpha_4,...,\alpha_N$固定。且
$$
y_1\alpha_1+y_2\alpha_2=-\sum_{i=3}^{N}y_i\alpha_i=\zeta
$$
相当于只有一个自由变量。SMO通过调整$\alpha_1(or\ \alpha_2)$的值，使得$\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)+\sum_{i=1}^N\alpha_i$值最小。
###  两个变量二次规划的求解方法
现在选定$\alpha_1,\alpha_2$为要更新的变量，记更新前的值为$\alpha_1^{old},\alpha_2^{old}$，显然
$$
y_1\alpha_1^{old}+y_2\alpha_2^{old}=\zeta
$$
和为一个常量（其他$\alpha$已经固定）。更新后的变量为$\alpha_1^{new},\alpha_2^{new}$，其依然满足原始优化问题的约束，即
$$
\begin{aligned}
y_1\alpha_1^{new}+y_2\alpha_2^{new}=\zeta=y_1\alpha_1^{old}+y_2\alpha_2^{old}\\
0 \leq alpha_1^{new}\leq C\\
0 \leq alpha_2^{new}\leq C
\end{aligned}
$$
不妨设自由变量为$\alpha_2$，则$\alpha_1 = y_i(\zeta - y_2\alpha_2)$
##### $\alpha_2$的取值范围
以下分两种情况讨论 **$\alpha_2$的取值范围**
- 当$y_1==y_2$时，不论$y_1==y_2==1$或者$y_1==y_2==-1$，显然都有$\alpha_1^{new}+\alpha_2^{new}=\alpha_1^{old}+\alpha_2^{old}$。当$\alpha_1^{new}$取最小0时，$\alpha_2^{new}$取最大，为$\alpha_1^{old}+\alpha_2^{old}$，又有$\alpha_2^{new}\leq C$，所以$\alpha_2^{new}$最大值为
$\min(C, \alpha_1^{old}+\alpha_2^{old})$
当$\alpha_1^{new}$取最大C时，$\alpha_2^{new}$取最小，为$\alpha_1^{old}+\alpha_2^{old}-C$，又有$\alpha_2^{new}\geq 0$，所以$\alpha_2^{new}$最小值为
$\max(0, \alpha_1^{old}+\alpha_2^{old}-C)$
所以此时
$$
\max(0, \alpha_1^{old}+\alpha_2^{old}-C) \leq \alpha_2^{new} \leq  \min(C, \alpha_1^{old}+\alpha_2^{old})
$$
- 当$y_1!=y_2$时，不论$y_1==-1$或者$y_2==-1$，都有$\alpha_2^{new}-\alpha_1^{new}=\alpha_2^{old}-\alpha_1^{old}$。同理通过$0 \leq \alpha_1^{new} \leq C$和$0 \leq \alpha_2^{new} \leq C$约束得到
$$
\max(0, \alpha_2^{old}-\alpha_1^{old}) \leq \alpha_2^{new} \leq  \min(C, C+\alpha_2^{old}-\alpha_1^{old})
$$
#####  求解
选定$\alpha_1, \alpha_2$后，SMO的优化问题可以写成
$$
\begin{aligned}
&\min_{\alpha_1, \alpha_2}=\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+y_1y_2K_{12}\alpha_1\alpha_2-(\alpha_1-\alpha_2)+y_1\alpha_1\sum_{i=3}^Ny_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_iK_{i2}\\
&subject\ to\ \alpha_1y_1+\alpha_2y_2=\zeta=\alpha_1^{old}+\alpha_1^{old},
0 \leq \alpha_i \leq C
\end{aligned}
$$
其中$K_{i,j}\ represent\ K(x_i,x_j)$。将$\alpha_1 = y_i(\zeta - y_2\alpha_2)$代入上式，并对$\alpha_2$求导使其倒数为0得到
$$
\alpha_2=\alpha_2^{new}+\frac{y_2(E_1-E2)}{\eta}
$$
其中
$$
\begin{aligned}
E_i&=(\sum_{j=1}^N\alpha_jy_jK(x_j, x_i)+b)-y_i,\ i=1,2\\
\eta&=K_{11}+K_{22}-2K_{11}
\end{aligned}
$$
又有$\alpha_2$必须符合取值范围，记得出的$\alpha_2$为$\alpha_2^{tmp}$所以
$$
\alpha_2=\left\{
\begin{aligned} 
& H  &\alpha_2^{tmp} \geq H \\ 
&\alpha_2^{tmp}  &  [1-y_i(wx_i+b)>0\\
& L  &\alpha_2^{tmp} \leq L
\end{aligned} 
\right.
$$
其中$H,L$为上步求出的取值范围的上下限。
##### 变量选择
###### 选择第一个变量
第一个变量选择违反KKT规则最严重的点，即检查样本点$(x_i, x_j)$是否满足
$$
\begin{aligned} 
&y_ig(x_i)\geq1\ when\ \alpha=0\\
&y_ig(x_i) = 1\ when\ 0 \leq \alpha\leq C\\
&y_ig(x_i)\leq1\ when\ \alpha=C\\
\end{aligned} 
$$
如果不满足该条件的程度最大，则选取该$\alpha$为$\alpha_2$。
另外，遍历搜索时先搜索$0 \leq \alpha\leq C$的点，因为这些点是当前的支持向量。如果它们都满足KKT条件，那么再遍历整个训练集，如果整个训练集都满足，那么算法结束。
###### 选择第二个变量
第二个变量的选择标准是希望$\alpha_2$有足够大的变化，显然$\alpha_2$的变化取决于$\frac{y_2(E_1-E2)}{\eta}$，该值的大小取只和$E_1,E_2$有关，选定$\alpha_2$后，$E_2$已经确定，则遍历所有的$\alpha$选择使$|E_1-E_2|$最大的那个$\alpha$作为$\alpha_1$。为了方便计算，每个样本的$E_i$算好保存着。
###### 更新b和$E_i$
每次更新$\alpha_1,\alpha_2$后，重新计算$b\ and\ E_i$，以便下次循环。