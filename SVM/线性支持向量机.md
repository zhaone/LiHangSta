SVM——线性支持向量机&核技巧
==============================
>把这两个放在一起讲主要是这两个方法都是对线性可分支持向量机的扩展，其实两者都是针对**线性不可分数据集**的。线性支持向量机的数据集**大致上是线性可分**的，存在少部分样本点没法线性可分，而核技巧主要用于**提高样本特征维度**，让原本线性不可分的数据转换到更高维的特征空间，从而**在更高维度上线性可分**。

# 软间隔最大化
对于线性可分向量机，我们讲分类平面记作**分离超平面**，几何距离为1（支持向量所在的平行于分离超平面的平面）的平面，称为**间隔平面**。对于线性不可分数据集，我们允许其落在间隔平面另一侧（分离平面与间隔平面之间或者分离超平面另一侧）。不过对于这些点要进行相应的惩罚。
### 合页损失函数
对于样本点$(x_i,y_i)$，如果它落在本类间隔平面的错误一侧，其距离本类间隔平面的函数距离可以用$1-y_i(wx_i+b)$表示。
- 该值为负，说明该类分类正确，且在间隔平面本侧
- 该值在[0,1]之间，说明该类分类正确， 且在间隔平面与分离超平面之间
- 该值大于1，说明该类在分类超平面另一侧，分类错误
为此，我们对每个样本$(x_i,y_i)$点增加一个损失，为：
$$
\xi_i=\left\{
\begin{aligned} 
& 0  & [1-y_i(wx_i+b)] \leq0 \\ 
&1-y_i(wx_i+b)]  &  [1-y_i(wx_i+b)>0
\end{aligned} 
\right.
$$
上述的$\xi$表示，当样本分类正确且在间隔平面正确一侧时，损失为零，否则损失为其到隔平面的距离，$\xi_i$叫做样本i的松弛变量。
### 惩罚参数C
当加入了$\xi$之后，对于样本点$x_i,y_i$，如果在间隔平面本侧，$1-y_i(wx_i+b)]\leq0=\xi_i$，如果在间隔平面另一侧，则$1-y_i(wx_i+b)]=\xi_i$。所以此时的约束条件变为
$$
\begin{aligned}
1-y_i(wx_i+b)\leq \xi_i,\ i=1,...,N\\
\xi_i\geq 0
\end{aligned}
$$
所以新的优化目标可以写为
$$
\begin{aligned}
min\ &\frac{1}{2}||w||^2+\sum_{i=1}^N\xi_i\\
subject\ to\ &1-y_i(wx_i+b)\leq \xi_i,\ i=1,...,N\\
& \xi_i\geq 0,\ i=1,...,N\\
\end{aligned}
$$
为了控制对间隔平面另一侧的点的惩罚程度，我们为损失添加一个常数C>0，成为惩罚参数。并将优化目标函数写为
$$
\begin{aligned}
min\ &\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i\\
subject\ to\ &1-y_i(wx_i+b)\leq  \xi_i,\ i=1,...,N\\
& \xi_i\geq 0,\ i=1,...,N\\
\end{aligned}
$$
显然，当**C较小**的时候，$\sum_{i=1}^N\xi_i$对优化目标影响较小，$\xi_i$可以取值较大，此时模型对分类偏差的点容忍度较高；当**C较大**的时候，$\sum_{i=1}^N\xi_i$对优化目标影响较大，$\xi_i$需要取值较小，此时模型对分类偏差的点容忍度较低。特别的，当**C取$+\infty$** 时，必须保证$\xi_i$都为0，此时模型不允许在间隔平面另一侧点的存在，模型其实与线性可分支持向量机相同。
### 优化问题
对于上述的优化问题，其解法思路与线性可分支持向量机相同，只不过变量变为了$w,b,\xi$三个。经过变换得到的对偶问题是：
$$
\begin{aligned}
\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i\\
subject\ to\ \sum_{i=1}^N\alpha_iy_i=0\\
0\leq\alpha_i\leq C
\end{aligned}
$$
依然是通过后面的SMO算法求解这个问题，得到最后的结果。
# 核技巧
### 特征空间变换
上面的线性支持向量机可以解决少部分样本点不能线性可分的问题，但是对于下图
![9007a8711a750247adecbfc44c18dcd3.png](en-resource://database/548:1)
这种显然不可能在该维度线性可分的数据集，线性支持向量机就无能为力了。这时可以将样本点特征进行升维，例如假设上图中特征点为二维向量$(X^{(0)},X^{(1)})$，看起来一个圈可以将两个数据集分来，假设该圈的表达式为
$$
X^{(0)^2}+X^{(1)^2}+X^{(0)}X^{(1)}+X^{(0)}+X^{(1)}+b=0
$$
那么其实我们可以将该二维特征投影到一个五维空间上，该空间各个维度的值如下：
$$
\begin{aligned}
&R^{(0)}=X^{(0)^2}\\
&R^{(1)}=X^{(1)^2}\\
&R^{(2)}=X^{(0)}X^{(1)}\\
&R^{(3)}=X^{(0)}\\
&R^{(4)}=X^{(1)}
\end{aligned}
$$
该五维空间。。。画不出来，但是将这个五位空间投影到3维的图大概是这样的
![41f9a0c57d56a84de91377db9a3d2ca1.gif](en-resource://database/550:1)
可以看到在得到的新的特征空间上就可以采用线性支持向量机来进行分类，得到一个这样的分类超平面
$$
w_0R^{(0)}+w_1R^{(1)}+w_2R^{(2)}+w_3R^{(3)}+w_4R^{(4)}+b=0
$$
我们记从原本的特征空间样本点$x_i$转换到新特征空间的映射函数为$\phi(x_i)$。
### 核函数
观察SVM——线性可分支持向量机中的最后分类平面的分类公式$\sum_{i=1}^N\alpha_iy_i(x_i\cdot x)+b=0$。对于新特征空间，该分类公式可以写做
$$
\sum_{i=1}^N\alpha_iy_i(\phi(x_i)\cdot \phi(x))+b=0
$$
其实是需要计算$\phi(x_i)\cdot \phi(x)$，如果一存在一个函数
$$
K(x_i,x)=\phi(x_i)\cdot \phi(x)
$$
那么可以用$K(x_i,x)$计算替代原来的映射函数+内积运算，该函数就叫做**核函数**。该函数相当于隐式地进行了特征空间地变换并进行了新特征空间上的内积运算。分类平面可以表示为
$$
\sum_{i=1}^N\alpha_iy_iK(x_i,x)+b=0
$$
>**核函数充要条件**
>这里不加证明地说下核函数的充要条件，基本照抄书上。设$K:\mathcal{X}\times\mathcal{X}\to \mathcal{R}$，则$K(x,z)$为正定核函数的充要条件是，对于任意$x_i\in\mathcal{X},i=1,2,...,m$，$K(x,z)$对应的Gram矩阵$K=[K(x_i,x_j)]_{m\times n}$是半正定矩阵。

### 常用的核函数
##### 多项式核函数
$$K(x,z)=(x\cdot z+1)^p$$
##### 高斯核函数
$$K(x,z)=exp(-\frac{||x-z||^2}{2\delta^2})$$

##### 字符串核函数